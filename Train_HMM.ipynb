{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "import keras.preprocessing.text\n",
    "import os\n",
    "import numpy as np\n",
    "from IPython.display import HTML\n",
    "import random\n",
    "from HMM import unsupervised_HMM\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('data/dataset.txt', 'r')\n",
    "s = file.read()\n",
    "txt_list = keras.preprocessing.text.text_to_word_sequence(s,\n",
    "                                               filters='0123456789!\"#$%&()*+,./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n",
    "                                               lower=True,\n",
    "                                               split=' ')\n",
    "# print(txt_list)\n",
    "Tokenizer = keras.preprocessing.text.Tokenizer(num_words=None,\n",
    "                                   filters='0123456789!\"#$%&()*+,./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n",
    "                                   lower=True,\n",
    "                                   split=\" \",\n",
    "                                   char_level=False,\n",
    "                                   oov_token=None)\n",
    "Tokenizer.fit_on_texts(txt_list)\n",
    "sequences = Tokenizer.texts_to_sequences(txt_list)\n",
    "word_docs = Tokenizer.word_docs\n",
    "word_index = Tokenizer.word_index\n",
    "word_counts = Tokenizer.word_counts\n",
    "# print(word_counts)\n",
    "# occ = 0\n",
    "# for val in word_counts.values():\n",
    "#     if val == 1:\n",
    "#         occ += 1\n",
    "# print(occ)\n",
    "# print(len(word_counts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "syllable_dict = json.load(open('data/my_syllable_dict.json'))\n",
    "word_to_syllable_dict = json.load(open('data/word_to_syllable_dict.json'))\n",
    "rhyme_dict = json.load(open('data/my_rhyme_dict.json'))\n",
    "reverse_dict = dict(zip(word_index.values(), word_index.keys()))\n",
    "word_to_rhyme_dict = json.load(open('data/word_to_rhyme_dict.json'))\n",
    "# print(rhyme_dict)\n",
    "filtered_rhyme_dict = {}\n",
    "for key in rhyme_dict.keys():\n",
    "    if len(rhyme_dict[key]) > 4:\n",
    "        filtered_rhyme_dict[key] = rhyme_dict[key]\n",
    "filtered_word_to_rhyme_dict = {}\n",
    "for key in word_to_rhyme_dict.keys():\n",
    "    if word_to_rhyme_dict[key] in filtered_rhyme_dict:\n",
    "        filtered_word_to_rhyme_dict[key] = word_to_rhyme_dict[key]\n",
    "# print(filtered_word_to_rhyme_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sequence(file_name, seq_type, Tokenizer):\n",
    "    '''\n",
    "    returns X: a list of list, tokenized\n",
    "    '''\n",
    "    if seq_type == 'poem':\n",
    "        file = open(file_name, 'r')\n",
    "        s = file.read()\n",
    "        s_list = keras.preprocessing.text.text_to_word_sequence(s,\n",
    "                                                       filters='!\"#$%&()+,./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n",
    "                                                       lower=True,\n",
    "                                                       split=' ')\n",
    "        poem_list = []\n",
    "        for s in s_list:\n",
    "            if s == '1':\n",
    "                poem = []\n",
    "            elif s.isdigit() or '*' in s:\n",
    "                poem_list.append(poem)\n",
    "                poem = []\n",
    "            else:\n",
    "                poem.append(s)\n",
    "        poem_list.append(poem)\n",
    "        # print(poem_list)\n",
    "        token_list = []\n",
    "        for poem in poem_list:\n",
    "            real_list = []\n",
    "            temp_list = Tokenizer.texts_to_sequences(poem)\n",
    "            for num in temp_list:\n",
    "                real_list.append(num[0] - 1)\n",
    "            token_list.append(real_list)\n",
    "        return token_list\n",
    "    \n",
    "    if seq_type == 'line':\n",
    "        file = open('data/dataset.txt', 'r')\n",
    "        s = file.read()\n",
    "        we_list = s.split('\\n')\n",
    "        line_list = []\n",
    "        for w in we_list:\n",
    "            temp = keras.preprocessing.text.text_to_word_sequence(w,\n",
    "                                                       filters='*!\"#$%&()+,./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n",
    "                                                       lower=True,\n",
    "                                                       split=' ')\n",
    "            if len(temp) > 1:\n",
    "                line_list.append(temp)\n",
    "        # print(line_list)\n",
    "\n",
    "        token_list = []\n",
    "        for line in line_list:\n",
    "            real_list = []\n",
    "            temp_list = Tokenizer.texts_to_sequences(line)\n",
    "            for num in temp_list:\n",
    "                real_list.append(num[0] - 1)\n",
    "            token_list.append(real_list)\n",
    "        # print(token_list)\n",
    "        return token_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = generate_sequence('data/dataset.txt', 'line', Tokenizer)\n",
    "temp=[]\n",
    "for Xi in X:\n",
    "    Xi=list(reversed(Xi))\n",
    "    temp.append(Xi)\n",
    "X=temp\n",
    "# print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation(X, fold, state, iteration, lamda_A, lamda_O):\n",
    "    print('hidden state: %d' % state)\n",
    "    random.shuffle(X)\n",
    "    # get average length\n",
    "    sum = 0\n",
    "    max_ob = 0\n",
    "    for x in X:\n",
    "        sum += len(x)\n",
    "        if max(x) > max_ob:\n",
    "            max_ob = max(x)\n",
    "    avg = int(sum / len(X))\n",
    "    \n",
    "    # fold split and train\n",
    "    kf = KFold(n_splits=fold)\n",
    "    count = 1\n",
    "    max_prob = 0\n",
    "    sum_prob = 0\n",
    "    sum_count = 0\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        if count%2 == 0:\n",
    "            count+=1\n",
    "            continue\n",
    "        print('fold %d' % count)\n",
    "        count += 1\n",
    "        # print(X[train_index[0]:(train_index[-1]-1)])\n",
    "        X_train = []\n",
    "        for i in train_index:\n",
    "            X_train.append(X[i])\n",
    "        # print(X_train)\n",
    "        hmm = unsupervised_HMM(X_train, state, iteration, max_ob+1, lamda_A, lamda_O)\n",
    "        # print(hmm.A)\n",
    "        # print(hmm.O)\n",
    "        \n",
    "        # print generated sequence\n",
    "#         emission, states = hmm.generate_emission(avg)\n",
    "#         emission = reverse_check(word_index, emission)\n",
    "#         text = ''\n",
    "#         for e in emission:\n",
    "#             text = str(text + e + ' ')\n",
    "#         print('Sample Text:')\n",
    "#         print(text)\n",
    "        \n",
    "        # calculate validation probability\n",
    "        prob = 0\n",
    "        for i in test_index:\n",
    "            prob += hmm.probability_alphas(X[i])\n",
    "        prob = prob / len(test_index)\n",
    "        if prob > max_prob:\n",
    "            max_prob = prob\n",
    "        sum_prob += prob\n",
    "        sum_count += 1\n",
    "    print('Max Probability in Fold: %s' % max_prob)\n",
    "    print('Avg Probability in Fold: %s' % (sum_prob / sum_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reverse_check(word_index, emission):\n",
    "    reverse_dict = dict(zip(word_index.values(), word_index.keys()))\n",
    "    res = []\n",
    "    for e in emission:\n",
    "        res.append(reverse_dict[e+1])\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_hmm(X, state, iteration, lamda_A, lamda_O):\n",
    "    random.shuffle(X)\n",
    "    # get average length\n",
    "    sum = 0\n",
    "    max_ob = 0\n",
    "    for x in X:\n",
    "        sum += len(x)\n",
    "        if max(x) > max_ob:\n",
    "            max_ob = max(x)\n",
    "    avg = int(sum / len(X))\n",
    "    hmm = unsupervised_HMM(X, state, iteration, max_ob+1, lamda_A, lamda_O)\n",
    "    return hmm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden state: 2\n",
      "fold 1\n",
      "Iteration: 10\n",
      "Iteration: 20\n",
      "Iteration: 30\n",
      "Iteration: 40\n",
      "Iteration: 50\n",
      "fold 3\n",
      "Iteration: 10\n",
      "Iteration: 20\n",
      "Iteration: 30\n",
      "Iteration: 40\n",
      "Iteration: 50\n",
      "fold 5\n",
      "Iteration: 10\n",
      "Iteration: 20\n",
      "Iteration: 30\n",
      "Iteration: 40\n",
      "Iteration: 50\n",
      "fold 7\n",
      "Iteration: 10\n",
      "Iteration: 20\n",
      "Iteration: 30\n",
      "Iteration: 40\n",
      "Iteration: 50\n",
      "fold 9\n",
      "Iteration: 10\n",
      "Iteration: 20\n",
      "Iteration: 30\n",
      "Iteration: 40\n",
      "Iteration: 50\n",
      "Max Probability in Fold: 1.7656846045499672e-17\n",
      "Avg Probability in Fold: 4.114041274237296e-18\n",
      "hidden state: 4\n",
      "fold 1\n",
      "Iteration: 10\n",
      "Iteration: 20\n",
      "Iteration: 30\n",
      "Iteration: 40\n",
      "Iteration: 50\n",
      "fold 3\n",
      "Iteration: 10\n",
      "Iteration: 20\n",
      "Iteration: 30\n",
      "Iteration: 40\n",
      "Iteration: 50\n",
      "fold 5\n",
      "Iteration: 10\n",
      "Iteration: 20\n",
      "Iteration: 30\n",
      "Iteration: 40\n",
      "Iteration: 50\n",
      "fold 7\n",
      "Iteration: 10\n",
      "Iteration: 20\n",
      "Iteration: 30\n",
      "Iteration: 40\n",
      "Iteration: 50\n",
      "fold 9\n",
      "Iteration: 10\n",
      "Iteration: 20\n",
      "Iteration: 30\n",
      "Iteration: 40\n",
      "Iteration: 50\n",
      "Max Probability in Fold: 1.5624144523047992e-18\n",
      "Avg Probability in Fold: 8.472265069379482e-19\n",
      "hidden state: 6\n",
      "fold 1\n",
      "Iteration: 10\n",
      "Iteration: 20\n",
      "Iteration: 30\n",
      "Iteration: 40\n",
      "Iteration: 50\n",
      "fold 3\n",
      "Iteration: 10\n",
      "Iteration: 20\n",
      "Iteration: 30\n",
      "Iteration: 40\n",
      "Iteration: 50\n",
      "fold 5\n",
      "Iteration: 10\n",
      "Iteration: 20\n",
      "Iteration: 30\n",
      "Iteration: 40\n",
      "Iteration: 50\n",
      "fold 7\n",
      "Iteration: 10\n",
      "Iteration: 20\n",
      "Iteration: 30\n",
      "Iteration: 40\n",
      "Iteration: 50\n",
      "fold 9\n",
      "Iteration: 10\n",
      "Iteration: 20\n",
      "Iteration: 30\n",
      "Iteration: 40\n",
      "Iteration: 50\n",
      "Max Probability in Fold: 1.7469655694620938e-17\n",
      "Avg Probability in Fold: 3.974507579924931e-18\n",
      "hidden state: 8\n",
      "fold 1\n",
      "Iteration: 10\n",
      "Iteration: 20\n",
      "Iteration: 30\n",
      "Iteration: 40\n",
      "Iteration: 50\n",
      "fold 3\n",
      "Iteration: 10\n",
      "Iteration: 20\n",
      "Iteration: 30\n",
      "Iteration: 40\n",
      "Iteration: 50\n",
      "fold 5\n",
      "Iteration: 10\n",
      "Iteration: 20\n",
      "Iteration: 30\n",
      "Iteration: 40\n",
      "Iteration: 50\n",
      "fold 7\n",
      "Iteration: 10\n",
      "Iteration: 20\n",
      "Iteration: 30\n",
      "Iteration: 40\n",
      "Iteration: 50\n",
      "fold 9\n",
      "Iteration: 10\n",
      "Iteration: 20\n",
      "Iteration: 30\n",
      "Iteration: 40\n",
      "Iteration: 50\n",
      "Max Probability in Fold: 1.118316803617589e-18\n",
      "Avg Probability in Fold: 5.918207460701434e-19\n",
      "hidden state: 10\n",
      "fold 1\n",
      "Iteration: 10\n",
      "Iteration: 20\n",
      "Iteration: 30\n",
      "Iteration: 40\n",
      "Iteration: 50\n",
      "fold 3\n",
      "Iteration: 10\n",
      "Iteration: 20\n",
      "Iteration: 30\n",
      "Iteration: 40\n",
      "Iteration: 50\n",
      "fold 5\n",
      "Iteration: 10\n",
      "Iteration: 20\n",
      "Iteration: 30\n",
      "Iteration: 40\n",
      "Iteration: 50\n",
      "fold 7\n",
      "Iteration: 10\n",
      "Iteration: 20\n",
      "Iteration: 30\n",
      "Iteration: 40\n",
      "Iteration: 50\n",
      "fold 9\n",
      "Iteration: 10\n",
      "Iteration: 20\n",
      "Iteration: 30\n",
      "Iteration: 40\n",
      "Iteration: 50\n",
      "Max Probability in Fold: 1.0984403036104048e-18\n",
      "Avg Probability in Fold: 6.64413951406323e-19\n"
     ]
    }
   ],
   "source": [
    "num_state = [2, 4, 6, 8, 10]\n",
    "for state in num_state:\n",
    "    cross_validation(X, 10, state, 50, 10000, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 10\n",
      "Iteration: 20\n",
      "Iteration: 30\n",
      "Iteration: 40\n",
      "Iteration: 50\n",
      "Iteration: 60\n",
      "Iteration: 70\n",
      "Iteration: 80\n",
      "Iteration: 90\n",
      "Iteration: 100\n"
     ]
    }
   ],
   "source": [
    "hmm = train_hmm(X, 8, 100, 10000, 10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Poem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_line(hmm, temp_word_to_rhyme_dict, rhyme_list=None):\n",
    "    states = []\n",
    "    emission = []\n",
    "    words = []\n",
    "    \n",
    "    # choose starting state\n",
    "    if rhyme_list == None:\n",
    "        y0 = np.random.randint(hmm.L)\n",
    "        states.append(y0)\n",
    "        x0 = np.random.choice(hmm.D, p=hmm.O[y0])\n",
    "        x_word = reverse_dict[x0+1]\n",
    "        while x_word not in word_to_syllable_dict or x_word not in temp_word_to_rhyme_dict:\n",
    "            x0 = np.random.choice(hmm.D, p=hmm.O[y0])\n",
    "            x_word = reverse_dict[x0+1]\n",
    "        emission.append(x0)\n",
    "        words.append(x_word)\n",
    "        rhyme = temp_word_to_rhyme_dict[x_word]\n",
    "        # print(rhyme)\n",
    "        new_rhyme_list = rhyme_dict[rhyme]\n",
    "        new_rhyme_list.remove(x_word)\n",
    "        temp_dict = temp_word_to_rhyme_dict.copy()\n",
    "        for key in temp_dict.keys():\n",
    "            if temp_word_to_rhyme_dict[key] == rhyme:\n",
    "                temp_word_to_rhyme_dict.pop(key)\n",
    "            \n",
    "        # print(new_rhyme_list)\n",
    "    else:\n",
    "        y0 = np.random.randint(hmm.L)\n",
    "        states.append(y0)\n",
    "        index_pool = []\n",
    "        prob_pool = []\n",
    "        for word in rhyme_list:\n",
    "            # print(word_index[word])\n",
    "            index_pool.append(word_index[word]-1)\n",
    "            prob_pool.append(hmm.O[y0][word_index[word]-1])\n",
    "        sum_pool = sum(prob_pool)\n",
    "        for i in range(len(prob_pool)):\n",
    "            prob_pool[i] = prob_pool[i] / sum_pool\n",
    "        # print(prob_pool)\n",
    "        x0 = np.random.choice(index_pool, p=prob_pool)\n",
    "        x_word = reverse_dict[x0+1]\n",
    "        emission.append(x0)\n",
    "        words.append(x_word)\n",
    "        # print(x_word)\n",
    "        \n",
    "        \n",
    "    # generate middle word\n",
    "    index = 1\n",
    "    remain_syllable = 10 - int(word_to_syllable_dict[x_word][-1])\n",
    "    while remain_syllable > 3:\n",
    "        y_temp = np.random.choice(hmm.L, p=hmm.A[states[index-1]])\n",
    "        states.append(y_temp)\n",
    "        x_temp = np.random.choice(hmm.D, p=hmm.O[states[index-1]])\n",
    "        x_word = reverse_dict[x_temp+1]\n",
    "        while x_word not in word_to_syllable_dict:\n",
    "            x_temp = np.random.choice(hmm.D, p=hmm.O[y0])\n",
    "            x_word = reverse_dict[x_temp+1]\n",
    "        emission.append(x_temp)\n",
    "        words.append(x_word)\n",
    "        remain_syllable -= int(word_to_syllable_dict[x_word][-1])\n",
    "        index += 1\n",
    "        \n",
    "    # generate last word pool\n",
    "    word_pool = []\n",
    "    E_syllable = 'E' + str(remain_syllable)\n",
    "    # print(E_syllable)\n",
    "    if E_syllable in syllable_dict:\n",
    "        word_pool = word_pool + syllable_dict[E_syllable]\n",
    "    word_pool = word_pool + syllable_dict[str(remain_syllable)]\n",
    "    # print(word_pool)\n",
    "    \n",
    "    y_temp = np.random.choice(hmm.L, p=hmm.A[states[index-1]])\n",
    "    states.append(y_temp)\n",
    "    x_temp = np.random.choice(hmm.D, p=hmm.O[states[index-1]])\n",
    "    x_word = reverse_dict[x_temp+1]\n",
    "    while x_word not in word_pool:\n",
    "        x_temp = np.random.choice(hmm.D, p=hmm.O[states[index-1]])\n",
    "        x_word = reverse_dict[x_temp+1]\n",
    "    emission.append(x_temp)\n",
    "    words.append(x_word)\n",
    "    \n",
    "    final_emission = []\n",
    "    emission = list(reversed(emission))\n",
    "    final_emission.append(emission)\n",
    "    final_words = []\n",
    "    words = list(reversed(words))\n",
    "    final_words.append(words)\n",
    "    \n",
    "    if rhyme_list is None:\n",
    "        # print('enter!')\n",
    "        emission_1, words_1, temp_word_to_rhyme_dict = generate_line(hmm, temp_word_to_rhyme_dict, new_rhyme_list)\n",
    "        final_words.append(words_1[0])\n",
    "        final_emission.append(emission_1[0])\n",
    "        # emission = emission + emission_1\n",
    "        # words = words + words_1\n",
    "        \n",
    "\n",
    "    return final_emission, final_words, temp_word_to_rhyme_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idea plead heart a breathe veil sharpened\n",
      "issueless score weighs prayers quickly look for\n",
      "nothing disdaineth inquire imprisoned\n",
      "deceased eyes a seeing why spring more\n",
      "cuckoo expressed and love this resty bends\n",
      "legacy mask disabled awards\n",
      "celestial that yield thou write daily spends\n",
      "counterfeit or care the to art we wards\n",
      "dignified the monsters side direct gate\n",
      "entertain perceiv'st in and if although\n",
      "applying because my smell make mind hate\n",
      "offenders kindle true mind's wondrous so\n",
      "well-tuned some sweet for that sessions hide\n",
      "evermore beggared my zealous abide\n"
     ]
    }
   ],
   "source": [
    "temp_word_to_rhyme_dict = filtered_word_to_rhyme_dict\n",
    "kaigoo = []\n",
    "for i in range(7):\n",
    "    emission, words, temp_word_to_rhyme_dict = generate_line(hmm, temp_word_to_rhyme_dict)\n",
    "    kaigoo.append(words[0])\n",
    "    kaigoo.append(words[1])\n",
    "    # print(emission)\n",
    "    # print(words)\n",
    "kaigoo_2 = [kaigoo[0], kaigoo[2], kaigoo[1], kaigoo[3], kaigoo[4], kaigoo[6], kaigoo[5], kaigoo[7], kaigoo[8], kaigoo[10], \\\n",
    "           kaigoo[9], kaigoo[11], kaigoo[12], kaigoo[13]]\n",
    "# print(kaigoo_2)\n",
    "for k in kaigoo_2:\n",
    "    print(' '.join(k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
