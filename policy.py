import os
import random
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.autograd import Variable

class Generator(nn.Module):
    """Generator """
    def __init__(self, emb_dim, hidden_dim, num_words=None, use_cuda=True, layers=1, dropout=0.0):
        super(Generator, self).__init__()
        self.emb_dim = emb_dim
        self.hidden_dim = hidden_dim
        if num_words is None:
            self.num_words = emb_dim
        else:
            self.num_words = num_words
        self.use_cuda = use_cuda
        self.layers = layers
        self.gru = nn.GRU(emb_dim, hidden_dim, layers, batch_first=True, dropout=dropout)
        self.lstm = nn.LSTM(emb_dim, hidden_dim, batch_first=True)
        self.lin = nn.Linear(hidden_dim, emb_dim)
        self.softmax = nn.LogSoftmax()
        self.init_params()
    
    def forward(self, x):
        """
        Args:
            x: (batch_size, seq_len, num_chars), sequence of tokens generated by generator
        """
        emb = x
        h_0 = self.init_hidden(x.shape[0])
        output, h = self.gru(emb, h_0)
        pred = self.softmax(self.lin(output.contiguous().view(-1, self.hidden_dim)))
        return pred
    
    def get_hidden(self, x, h=None):
        emb = x
        output, h = self.gru(emb, h)
        return output[:, -1, :], h

    def step(self, x, h, T):
        """
        Args:
            x: (batch_size, 1, num_chars), sequence of tokens generated by generator
            h: (1, batch_size, hidden_dim), gru hidden state
        """
        emb = x
        output, h = self.gru(emb, h)
        pred = F.softmax(self.lin(output.view(-1, self.hidden_dim)) / T)
        return pred, h

    def init_hidden(self, batch_size):
        h = Variable(torch.zeros((self.layers, batch_size, self.hidden_dim)).double())
        if self.use_cuda:
            h = h.cuda()
        return h
    
    def init_params(self):
        for param in self.parameters():
            param.data.uniform_(-0.05, 0.05)

    def sample(self, max_seq_len, stop_sign, T, seed=None):
    
        count = 0
        if seed is None:
            samples = []
            x = Variable(torch.zeros(1, 1, self.emb_dim).double()).cuda()
            x[0, 0, 0] = 1.0
            
            if self.use_cuda:
                x = x.cuda()
            h = self.init_hidden(1)
            for i in range(max_seq_len):
                output, h = self.step(x, h, T)
                num = output.multinomial(1).squeeze().data[0]
                if num == stop_sign:
                    count += 1
                    if count == 14:
                        return samples
                samples.append(num)
                x = Variable(torch.zeros(1, 1, self.emb_dim).double()).cuda()
                x[0, 0, num] = 1.0
            
        else:
            h = self.init_hidden(1)
            samples = []
            for i in range(len(seed)):
                if seed[i] == 0:
                    count += 1
                x = Variable(torch.zeros(1, 1, self.emb_dim).double()).cuda()
                x[0, 0, seed[i]] = 1.0
                output, h = self.step(x, h, T)
                samples.append(seed[i])
            
            num = output.multinomial(1).long().squeeze().data[0]
            samples.append(num)
            x = Variable(torch.zeros(1, 1, self.emb_dim).double()).cuda()
            x[0, 0, num] = 1.0
            for i in range(len(seed), max_seq_len):
                output, h = self.step(x, h, T)
                num = output.multinomial(1).squeeze().data[0]
                if num == stop_sign:
                    count += 1
                    if count == 14:
                        return samples
                samples.append(num)
                x = Variable(torch.zeros(1, 1, self.emb_dim).double()).cuda()
                x[0, 0, num] = 1.0
    
    
        
class Feature_Generator(nn.Module):
    """Word_Feature_Generator """
    def __init__(self, emb_dim, hidden_dim, vectors, num_words, use_cuda=True, layers=1, dropout=0.0):
        super(Feature_Generator, self).__init__()
        self.emb_dim = emb_dim
        self.hidden_dim = hidden_dim
        self.vectors = vectors
        self.num_words = num_words
        self.use_cuda = use_cuda
        self.layers = layers
        self.gru = nn.GRU(emb_dim, hidden_dim, layers, batch_first=True, dropout=dropout)
        self.lstm = nn.LSTM(emb_dim, hidden_dim, batch_first=True)
        self.lin = nn.Linear(hidden_dim, self.num_words)
        self.softmax = nn.LogSoftmax()
        self.init_params()
    
    def forward(self, x):
        """
        Args:
            x: (batch_size, seq_len, emb_dim)
        """
        emb = x
        h_0 = self.init_hidden(x.shape[0])
        output, h = self.gru(emb, h_0)
        pred = self.softmax(self.lin(output.contiguous().view(-1, self.hidden_dim)))
        return pred
    
    def get_hidden(self, x, h=None):
        emb = x
        output, h = self.gru(emb, h)
        return output[:, -1, :], h

    def step(self, x, h, T):
        """
        Args:
            x: (batch_size, 1, emb_dim)
            h: (1, batch_size, hidden_dim), gru hidden state
        """
        emb = x
        output, h = self.gru(emb, h)
        pred = F.softmax(self.lin(output.view(-1, self.hidden_dim)) / T)
        return pred, h

    def init_hidden(self, batch_size):
        h = Variable(torch.zeros((self.layers, batch_size, self.hidden_dim)).double())
        if self.use_cuda:
            h = h.cuda()
        return h
    
    def init_params(self):
        for param in self.parameters():
            param.data.uniform_(-0.05, 0.05)

    def sample(self, max_seq_len, stop_sign, T, seed):
        count = 0
        h = self.init_hidden(1)
        samples = []
        for i in range(len(seed)):
            if seed[i] == 0:
                count += 1
            x = Variable(torch.zeros(1, 1, self.emb_dim).double()).cuda()
            x[0, 0] = Variable(torch.cuda.DoubleTensor(self.vectors[seed[i]]))
            output, h = self.step(x, h, T)
            samples.append(seed[i])
        
        num = output.multinomial(1).long().squeeze().data[0]
        samples.append(num)
        x = Variable(torch.zeros(1, 1, self.emb_dim).double()).cuda()
        x[0, 0] = Variable(torch.cuda.DoubleTensor(self.vectors[num]))
        for i in range(len(seed), max_seq_len):
            output, h = self.step(x, h, T)
            num = output.multinomial(1).squeeze().data[0]
            if num == stop_sign:
                count += 1
                if count == 14:
                    return samples
            samples.append(num)
            x = Variable(torch.zeros(1, 1, self.emb_dim).double()).cuda()
            x[0, 0] = Variable(torch.cuda.DoubleTensor(self.vectors[num]))